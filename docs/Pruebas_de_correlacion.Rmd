---
output: html_document
header-includes:
  - \usepackage{float}
  - \floatstyle{boxed}
  - \restylefloat{figure}
---

<div style="display:flex; align-items:center;">
  <h1 style="margin:0;">Pruebas de Correlación</h1>
  <img src="Imagenes/HNBI_LOGO1.jpeg" alt="Texto alternativo de la imagen" style="width:100px; margin-left:20px;">
</div>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

Las pruebas de correlación sirven para determinar la tendencia o el comportamiento que hay entre dos variables cuando están juntas.

<br>

#### Paquetes

```{r, message=FALSE, warning=FALSE}
library(rio)
library(tidyverse)
library(GGally)
library(correlation)
library(see)
library(inspectdf)
```

<br>

#### Datos

```{r}
suelos <- import("data/suelos.xlsx")
head(suelos)
```

<br>

Podemos explorar gráficamente si dos variables de nuestra tabla de datos están relacionadas o no.

```{r}
ggplot(suelos, aes(K, Mg)) +
  geom_point()

# con un gráfico de dispersión podemos ver si existe un patrón entre las dos variables, por ejemplo, en este gráfico se puede apreciar que no hay un patrón de relación específico entre el potasio (K) y el magnesio (Mg) según los datos de la tabla `suelos`.
```

```{r}
ggplot(suelos, aes(Na, Conduc)) +
  geom_point()
# En este gráfico se puede apreciar un patrón claro de la relación entre las variables, a mayores concentraciones de sodio (Na) hay mayor conductividad (Conduc) según los datos de la tabla `suelos`.
```

Pero si queremos obtener un valor numérico de esa relación debemos utilizar pruebas de correlación.

<br>

### Contenido{.tabset .tabset-pills}

<br>

#### Prueba de correlación de Pearson 

Es un método paramétrico para determinar la correlación entre dos variables.
Para utilizar la correlación de Pearson, nuestros datos deben cumplir el supuesto de normalidad.

```{r}
shapiro.test(suelos$K)
```

```{r}
shapiro.test(suelos$Mg)
```

Una vez confirmada la normalidad de las dos variables, se ejecuta la función `cor.test()`.

```{r}
cor.test(suelos$K, suelos$Mg) # Aquí no importa el orden en el que coloquemos las variables.
```

El valor de p indica la significancia estadística de la correlación entre las dos variables.
Si el valor de p < 0.05 = La correlación entre las variables es estadísticamente significativa.
Si el valor de p >= 0.05 = La correlación entre las variables NO es estadísticamente significativa.

El valor de `cor` indica la magnitud y dirección de la correlación, ya que la correlación puede ser positiva o negativa. En este caso, las variables de K y Mg están negativamente correlacionadas (cor = -0.36). Los valores de `cor` pueden ir de -1 a 1, entre más cercano a -1 o 1 sea el valor de `cor`, más fuerte es la correlación entre las variables, entre más cercano a 0 (positiva o negativamente), la correlación es más débil. 

El intervalo de confianza al 95% nos indica que la verdadera correlación entre las variables podría estar entre -0.58 y -0.08.

La correlación de Pearson es una medida de la dependencia lineal entre las dos variables. **Si la relación de una variable con la otra no es lineal, la correlación será baja.**

<br>

#### Prueba de correlación de Spearman 

Si una de las variables no cumple con el supuesto de normalidad, se puede utilizar una prueba no paramétrica, la **prueba de correlación de Spearman**.

```{r}
shapiro.test(suelos$Na)
```

```{r}
shapiro.test(suelos$Conduc)
```

Para la prueba de Spearman se utiliza la misma función `cor.test()`, pero se especifica el `method = "spearman"`.

```{r, message=FALSE, warning=FALSE}
cor.test(suelos$Na, suelos$Conduc, method = "spearman")
```

En este caso, el valor de `rho` indica la magnitud y dirección de la correlación (equivalente al valor de `cor` en la prueba de Pearson). En este ejemplo hay una correlación positiva muy fuerte entre las variables Na y Conduc (0.96).

La prueba de Spearman es menos robusta que la de Pearson porque no es específica para un tipo de distribución de los datos. También podemos aplicar la prueba de Pearson si los datos no cumplen el supuesto de normalidad, aunque esta pierde un poco de robustez.

<br>

#### Correlación entre múltiples variables

También podemos evaluar la correlación entre más de dos variables

Podemos utilizar la función `ggpairs()` del paquete `GGally`. Aunque para esta función debemos seleccionar solo las variables numéricas de nuestra base de datos.

```{r}
suelosb <- suelos[,6:14] # seleccionamos de la columna 6 a la 14 que son las que contienen las variables numéricas que queremos evaluar
head(suelosb)
```

```{r, message=FALSE}
ggpairs(suelosb)

# El resultado nos muestra una matriz diagonal. La diagonal muestra gráficos de densidad con la distribución de cada variable (individualmente), bajo la diagonal se muestran gráficos de dispersión de la relación entre todas las variables (una contra otra), y sobre la diagonal se muestran los valores de correlación entre las variables y la significancia estadísitica de la correlación (en simbología).
```

Para observar más detalladamente podemos seleccionar solo algunas variables de interés:

```{r, message=FALSE, warning=FALSE}
ggpairs(suelosb, c("pH", "N", "P", "K"))
```

Este resultado es compatible con `ggplot2` y podemos guardarlo como imagen:

```{r}
ggsave("Imagenes/correlacion.jpg")
```

<br>

También podemos evaluar la correlación entre las variables con la función `correlation()` del paquete `correlation`. Para esta función no es necesario seleccionar las variables numéricas de nuestra base de datos.

```{r}
c <- correlation(suelos)  
c

# Aquí, `r` = correlación de Pearson.
# Si el IC95 no contiene (traslapa) el 0, la correlación es significativa
```

Podemos exportar este resultado como una tabla de la siguiente manera:

```{r}
cor <- data.frame(correlation(suelos))
export(cor,"Resultados/correlacionsuelos.xlsx")
```

También podemos graficarlo. Para esto generamos un resumen de las correlaciones entre las variables:

```{r}
sc <- summary(c) # para generar una matriz de correlación
sc
```

Y graficamos con la función `plot()` del paquete `see`.

```{r}
plot(sc)

# El resultado nos muestra una matriz de correlación de una forma más gráfica, los colores indican la dirección y la magnitud de la correlación.
```

```{r}
ggsave("Imagenes/corr.jpg")
```

También podemos generar esa matriz gráfica de correlación de la siguiente manera:

```{r, message=FALSE, warning=FALSE}
c <- as.table(c) # para hacer la matriz de correlación 
c
```

```{r}
plot(c, type = "tile", show_values= TRUE, show_p= TRUE)
```

También podemos especificar el método que se utilice para realizar la correlación con el argumento `method =`.

```{r}
cor_spearman <- correlation(suelos, method = "spearman")
cor_spearman
```


<br>

Otra forma de evaluar la correlación entre variables es con la función `inspect_cor()` del paquete `inspectdf`

```{r}
ic <- inspect_cor(suelos)
ic
```

Y esto también podemos graficarlo incluyendo los intervalos de confianza al 95% con la función `show_plot()` del paquete `inspectdf`.

```{r}
cor_suelos <- show_plot(ic)
cor_suelos
# Como se puede observar, todas las correlaciones de esta base de datos son significativas, ya que ningún intervalo de confianza traslapa con el 0.
```

```{r, message=FALSE, warning=FALSE}
ggsave("Imagenes/cor_suelos.jpg")
```

Un ejemplo de correlaciones no significativas:

```{r}
cor_mtcars <- inspect_cor(mtcars) # "mtcars" es una base de datos del paquete base de R que contiene datos de pruebas en carretera de diferentes motores de automóviles.

show_plot(cor_mtcars)
```

<br>