---
output: html_document
header-includes:
  - \usepackage{float}
  - \floatstyle{boxed}
  - \restylefloat{figure}
---

<div style="display:flex; align-items:center;">
  <h1 style="margin:0;">Pruebas de Correlación</h1>
  <img src="Imagenes/HNBI_LOGO1.jpeg" alt="Texto alternativo de la imagen" style="width:100px; margin-left:20px;">
</div>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<br>

Las pruebas de correlación sirven para determinar si dos variables están relacionadas o no.

<br>

#### Paquetes

```{r, message=FALSE, warning=FALSE}
library(rio)
library(tidyverse)
library(GGally)
library(correlation)
library(see)
library(inspectdf)
```

<br>

#### Datos

```{r}
suelos <- import("data/suelos.xlsx")
head(suelos)
```

<br>

Podemos explorar gráficamente si dos variables de nuestra tabla de datos están relacionadas o no.

```{r}
ggplot(suelos, aes(K, Mg)) +
  geom_point()

# con un gráfico de dispersión podemos ver si existe un patrón entre las dos variables, por ejemplo, en este gráfico se puede apreciar que no hay un patrón de relación específico entre el potasio (K) y el magnesio (Mg) según los datos de la tabla `suelos`.
```

```{r}
ggplot(suelos, aes(Na, Conduc)) +
  geom_point()
# En este gráfico se puede apreciar un patrón claro de la relación entre las variables, a mayores concentraciones de sodio (Na) hay mayor conductividad (Conduc) según los datos de la tabla `suelos`.
```

Pero si queremos obtener un valor numérico de esa relación debemos utilizar pruebas de correlación.

<br>

### Contenido{.tabset .tabset-pills}

<br>

#### Prueba de correlación de Pearson 

Es un método paramétrico para determinar la correlación entre dos variables.
Para utilizar la correlación de Pearson, nuestros datos deben cumplir el supuesto de normalidad.

```{r}
shapiro.test(suelos$K)
```

```{r}
shapiro.test(suelos$Mg)
```

Una vez confirmada la normalidad de las dos variables, se ejecuta la función `cor.test()`.

```{r}
cor.test(suelos$K, suelos$Mg) # Aquí no importa el orden en el que coloquemos las variables.
```

El valor de p indica la significancia estadística de la correlación entre las dos variables.
Si el valor de p < 0.05 = La correlación entre las variables es estadísticamente significativa.
Si el valor de p >= 0.05 = La correlación entre las variables NO es estadísticamente significativa.

El valor de `cor` indica la magnitud y dirección de la correlación, ya que la correlación puede ser positiva o negativa. En este caso, las variables de K y Mg están negativamente correlacionadas (cor = -0.36). Los valores de `cor` pueden ir de -1 a 1, entre más cercano a -1 o 1 sea el valor de `cor`, más fuerte es la correlación entre las variables, entre más cercano a 0 (positiva o negativamente), la correlación es más débil. **Si la relación de una variable con la otra no es lineal, la correlación será baja.**

El intervalo de confianza al 95% nos indica que la verdadera correlación entre las variables podría estar entre -0.58 y -0.08.


#### Prueba de correlación de Spearman 

Si una de las variables no cumple con el supuesto de normalidad, se puede utilizar una prueba no paramétrica, la **prueba de correlación de Spearman**.

```{r}
shapiro.test(suelos$Na)
```

```{r}
shapiro.test(suelos$Conduc)
```

Para la prueba de Spearman se utiliza la misma función `cor.test()`, pero se especifica el `method = "spearman"`.

```{r, message=FALSE, warning=FALSE}
cor.test(suelos$Na, suelos$Conduc, method = "spearman")
```

En este caso, el valor de `rho` indica la magnitud y dirección de la correlación (equivalente al valor de `cor` en la prueba de Pearson). En este ejemplo hay una correlación positiva muy fuerte entre las variables Na y Conduc (0.96).

La prueba de Spearman es menos robusta que la de Pearson porque no es específica para un tipo de distribución de los datos. También podemos aplicar la prueba de Pearson si los datos no cumplen el supuesto de normalidad, aunque esta pierde un poco de robustez.

<br>

#### Correlación entre múltiples variables

También podemos evaluar la correlación entre más de dos variables

Podemos utilizar la función `ggpairs()` del paquete `GGally`. Aunque para esta función debemos seleccionar solo las variables numéricas de nuestra base de datos.

```{r}
suelosb <- suelos[,6:14] # seleccionamos de la columna 6 a la 14 que son las que contienen las variables numéricas que queremos evaluar
head(suelosb)
```

```{r, message=FALSE}
ggpairs(suelosb)
```

```{r, message=FALSE, warning=FALSE}
ggpairs(suelosb, c("pH", "N", "P", "K"))
```


También podemos evaluar la correlación entre las variables con la función `correlation()` del paquete `correlation`

```{r}
c <- correlation(suelos)
c
```

Aquí, `r` = correlación de Pearson.
si el IC95 no contiene (traslapa) el 0, la correlación es significativa

También podemos graficarlo. Para esto generamos un resumen de las correlaciones entre las variables:

```{r}
sc <- summary(c)
sc
```

Y graficamos con la función `plot()` del paquete `see`.

```{r}
plot(sc)
```

```{r}
ggsave("Imagenes/corr.jpg")
```

```{r}
c %>%
  as.table()%>%
  plot(type = "tile", show_values= TRUE, show_p= TRUE)
```

Otra forma de evaluar la correlación entre variables es con la función `inspect_cor()` del paquete `inspectdf`

```{r}
ic <- inspect_cor(suelos)
ic
```

Y esto también podemos graficarlo incluyendo los intervalos de confianza al 95% con la función `show_plot()` del paquete `inspectdf`.

```{r}
show_plot(ic)
```

Como se puede observar, todas las correlaciones de esta base de datos son significativas, ya que ningún intervalo de confianza traslapa con el 0.

Un ejemplo de correlaciones no significativas:

```{r}
inspect_cor(mtcars) |>
  show_plot()
```